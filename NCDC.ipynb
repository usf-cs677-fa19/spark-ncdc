{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(WBANNO,FloatType,true),StructField(UTC_DATE,FloatType,true),StructField(UTC_TIME,FloatType,true),StructField(LST_DATE,FloatType,true),StructField(LST_TIME,FloatType,true),StructField(CRX_VN,FloatType,true),StructField(LONGITUDE,FloatType,true),StructField(LATITUDE,FloatType,true),StructField(AIR_TEMPERATURE,FloatType,true),StructField(PRECIPITATION,FloatType,true),StructField(SOLAR_RADIATION,FloatType,true),StructField(SR_FLAG,FloatType,true),StructField(SURFACE_TEMPERATURE,FloatType,true),StructField(ST_TYPE,FloatType,true),StructField(ST_FLAG,FloatType,true),StructField(RELATIVE_HUMIDITY,FloatType,true),StructField(RH_FLAG,FloatType,true),StructField(SOIL_MOISTURE_5,FloatType,true),StructField(SOIL_TEMPERATURE_5,FloatType,true),StructField(WETNESS,FloatType,true),StructField(WET_FLAG,FloatType,true),StructField(WIND_1_5,FloatType,true),StructField(WIND_FLAG,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    feats.append(StructField(line.strip(), FloatType(), True))\n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our space-separated file into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(WBANNO=94088.0, UTC_DATE=20180100.0, UTC_TIME=5.0, LST_DATE=20171232.0, LST_TIME=1705.0, CRX_VN=2.0, LONGITUDE=-104.44000244140625, LATITUDE=44.52000045776367, AIR_TEMPERATURE=-20.0, PRECIPITATION=0.0, SOLAR_RADIATION=0.0, SR_FLAG=0.0, SURFACE_TEMPERATURE=-19.299999237060547, ST_TYPE=0.0, ST_FLAG=0.0, RELATIVE_HUMIDITY=83.0, RH_FLAG=0.0, SOIL_MOISTURE_5=-99.0, SOIL_TEMPERATURE_5=-0.800000011920929, WETNESS=1094.0, WET_FLAG=0.0, WIND_1_5=1.0199999809265137, WIND_FLAG=0.0),\n",
       " Row(WBANNO=94088.0, UTC_DATE=20180100.0, UTC_TIME=10.0, LST_DATE=20171232.0, LST_TIME=1710.0, CRX_VN=2.0, LONGITUDE=-104.44000244140625, LATITUDE=44.52000045776367, AIR_TEMPERATURE=-20.100000381469727, PRECIPITATION=0.0, SOLAR_RADIATION=0.0, SR_FLAG=0.0, SURFACE_TEMPERATURE=-19.299999237060547, ST_TYPE=0.0, ST_FLAG=0.0, RELATIVE_HUMIDITY=83.0, RH_FLAG=0.0, SOIL_MOISTURE_5=-99.0, SOIL_TEMPERATURE_5=-0.800000011920929, WETNESS=1094.0, WET_FLAG=0.0, WIND_1_5=0.6600000262260437, WIND_FLAG=0.0),\n",
       " Row(WBANNO=94088.0, UTC_DATE=20180100.0, UTC_TIME=15.0, LST_DATE=20171232.0, LST_TIME=1715.0, CRX_VN=2.0, LONGITUDE=-104.44000244140625, LATITUDE=44.52000045776367, AIR_TEMPERATURE=-20.100000381469727, PRECIPITATION=0.0, SOLAR_RADIATION=0.0, SR_FLAG=0.0, SURFACE_TEMPERATURE=-19.5, ST_TYPE=0.0, ST_FLAG=0.0, RELATIVE_HUMIDITY=83.0, RH_FLAG=0.0, SOIL_MOISTURE_5=-99.0, SOIL_TEMPERATURE_5=-0.800000011920929, WETNESS=1094.0, WET_FLAG=0.0, WIND_1_5=0.3799999952316284, WIND_FLAG=0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# If you had a regular CSV or TDV file you could read it like so:\n",
    "#df = spark.read.format('csv').option('sep', ' ').schema(schema).load('/Volumes/hdd/Datasets/some/where/*')\n",
    "\n",
    "\n",
    "txt_rdd = sc.textFile('hdfs://orion12:9001/ncdc/2018/CRNS0101-05-2018-WY_Sundance_8_NNW.txt')\n",
    "\n",
    "def conv(string):\n",
    "    try:\n",
    "        return float(string)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df = txt_rdd.map(lambda row : [conv(r) for r in row.split()]).toDF(schema)\n",
    "\n",
    "df.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105059"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "really_hot = df.filter(df.SURFACE_TEMPERATURE > 20).count()\n",
    "print(really_hot)\n",
    "\n",
    "df.filter(df.WETNESS > 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=83.0)\n",
      "Row(RELATIVE_HUMIDITY=82.0)\n",
      "Row(RELATIVE_HUMIDITY=82.0)\n",
      "Row(RELATIVE_HUMIDITY=82.0)\n",
      "Row(RELATIVE_HUMIDITY=82.0)\n",
      "Max val observed: [Row(maxval=99.0)]\n"
     ]
    }
   ],
   "source": [
    "# Creating an SQL 'table'\n",
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "hum = spark.sql(\"SELECT RELATIVE_HUMIDITY FROM TEMP_DF\").collect()\n",
    "# .collect() gives us a list of rows. Let's grab the first 10:\n",
    "for i in range(10):\n",
    "    print(hum[i])\n",
    "\n",
    "# What's the maximum value?\n",
    "hummax = spark.sql(\"SELECT MAX(RELATIVE_HUMIDITY) as maxval FROM TEMP_DF\").collect()\n",
    "\n",
    "print('Max val observed:', hummax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06b8b8c226ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWETNESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(df.WETNESS)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a sample dataset with Spark! Let's create a 10% sample (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(False, .1)\n",
    "\n",
    "# Write it out\n",
    "samp.write.format('csv').save('hdfs://orion12:50000/sampled_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
